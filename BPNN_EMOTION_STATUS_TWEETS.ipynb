{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_FINAL EXAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9a3k-FeAIUi"
      },
      "source": [
        "# Importing Libraries dan Membuat Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_LVtSQdIa-r"
      },
      "source": [
        "#importing libraries\n",
        "# untuk tensorflow yang dipakai menggunakan versi 1.15 untuk melakukan bacpropagation\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dDKCFtpQ1XT"
      },
      "source": [
        "#membentuk dataset\n",
        "df = [(\"I feel like I am drowning. #depression #anxiety #failure #worthless\", \"fear\"),\n",
        "      (\"#panic Panic attack from fear of starting new medication\", \"fear\"),\n",
        "      (\"My bus was in a car crash... I'm still shaking a bit... This week was an absolute horror and this was the icing on the cake... #terrible\", \"fear\"),\n",
        "      (\"Just got back from seeing @GaryDelaney in Burslem. AMAZING!! Face still hurts from laughing so much #hilarious\", \"joy\"),\n",
        "      (\"It's the #FirstDayofFall and I'm so happy. Sipping my #PumpkinSpice flavoured coffee and #smiling! Happy Fall everyone! #amwriting\", \"joy\"),\n",
        "      (\"Morning all! Of course it is sunny on this Monday morning to cheerfully welcome us back to work.:)\", \"joy\")]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iURh5EesS44h"
      },
      "source": [
        "#dataset yang dibentuk akan di apply ke dalam dataframe menggunakan pandas\n",
        "df = pd.DataFrame(df, columns=['Data', 'Label'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "sYc7V2gRUIS6",
        "outputId": "58e10038-7fd5-4374-f749-dfeb44f6a410"
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I feel like I am drowning. #depression #anxiet...</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#panic Panic attack from fear of starting new ...</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My bus was in a car crash... I'm still shaking...</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It's the #FirstDayofFall and I'm so happy. Sip...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Morning all! Of course it is sunny on this Mon...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Data Label\n",
              "0  I feel like I am drowning. #depression #anxiet...  fear\n",
              "1  #panic Panic attack from fear of starting new ...  fear\n",
              "2  My bus was in a car crash... I'm still shaking...  fear\n",
              "3  Just got back from seeing @GaryDelaney in Burs...   joy\n",
              "4  It's the #FirstDayofFall and I'm so happy. Sip...   joy\n",
              "5  Morning all! Of course it is sunny on this Mon...   joy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbvFHo8UA0q1"
      },
      "source": [
        "#DATA PREPROCESSING FOR CLEANING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8X0Bic4UNgW"
      },
      "source": [
        "#mengimport libraries untuk cleaning data\n",
        "#menggunakan dan mendownload stopwords WordNetLemmatizer dan re untuk preprocess data\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aI1ZwywVpQI",
        "outputId": "6418d4a4-055a-42c2-e3dd-387dc166e309"
      },
      "source": [
        "#download stopwords\n",
        "nltk.download('stopwords')\n",
        "#download wordnet\n",
        "nltk.download('wordnet')\n",
        "#download punctuation\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbdaFuBkVvJu"
      },
      "source": [
        "#membentuk object cleanedWords untuk menampung kalimat/string yang akan di preprocess\n",
        "cleanedWords = []\n",
        "#inisialisasi WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "#inisialisasi stopwords\n",
        "st_words = set(stopwords.words(\"english\"))\n",
        "# membuat variabel tx untuk menampung seluruh string yang ada di dalam dataset(df[Data])\n",
        "for tx in df['Data']:\n",
        "  #mengubah seluruh string yang memiliki huruf kapital menjadi huruf kecil\n",
        "  tx = tx.lower()\n",
        "  #membersihkan atau menghapus special character\n",
        "  tx = re.sub(\"[^A-Za-z0-9]+\",\" \",tx)\n",
        "  #tokenizing and lemmatizing\n",
        "  #tokenizing: membagi text menjadi words (membagi kalimat menjadi kata)\n",
        "  tx = nltk.word_tokenize(tx.lower())\n",
        "  #lemmatizing: mengubah words atau kata-kata yang ada di dataset ke bentuk base form nya contohnya kata hurts yang di lemmatizing akan menghasilkan hurt.\n",
        "  tx = [lemm.lemmatize(word) for word in tx]\n",
        "  #remove stopwords\n",
        "  tx = [word for word in tx if word not in st_words]\n",
        "  #joining string yang sudah di proses\n",
        "  tx = \" \".join(tx)\n",
        "\n",
        "  cleanedWords.append(tx)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NgvpC6zaYjf",
        "outputId": "d32988f7-4a25-4cdb-fca7-71dba0334453"
      },
      "source": [
        "#melihat hasil string yang sudah di preprocess\n",
        "ranging_scale = 6\n",
        "for i in range(ranging_scale):\n",
        "    print(cleanedWords[i],end=\"\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feel like drowning depression anxiety failure worthless\n",
            "panic panic attack fear starting new medication\n",
            "bus wa car crash still shaking bit week wa absolute horror wa icing cake terrible\n",
            "got back seeing garydelaney burslem amazing face still hurt laughing much hilarious\n",
            "firstdayoffall happy sipping pumpkinspice flavoured coffee smiling happy fall everyone amwriting\n",
            "morning course sunny monday morning cheerfully welcome u back work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PwtX3-YbT41"
      },
      "source": [
        "#merubah label false dan joy ke 0 dan 1\n",
        "df['Label'] = df['Label'].replace(['fear', 'joy'], [0,1])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNqXENW2ebsK",
        "outputId": "e34f5c90-6a30-4c54-f386-63c6e9ad2089"
      },
      "source": [
        "df['Label'].head(6)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    1\n",
              "4    1\n",
              "5    1\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3viZLkeENHW"
      },
      "source": [
        "# WORD2VEC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQu-YB-Vgim1"
      },
      "source": [
        "#melakukan import libraries gensim dan Word2vec serta mengapply cleanedWords ke dalam model Word2vec\n",
        "import warnings as wr\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "wr.filterwarnings(action='ignore')\n",
        "\n",
        "w_model = gensim.models.Word2Vec(cleanedWords, min_count =1, window=5)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLwT4OHHkt_d",
        "outputId": "dbf10dea-83e2-494b-cb34-7d44a8d15dd2"
      },
      "source": [
        "# print model untuk mengecek size, vocab dan alpha\n",
        "print(w_model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=25, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4aYSb-lL3F",
        "outputId": "7bdb116f-8747-4e11-9b30-9a96044b2864"
      },
      "source": [
        "#list vocab yang terkandung di dalam model\n",
        "words = list(w_model.wv.vocab)\n",
        "print(words)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['f', 'e', 'l', ' ', 'i', 'k', 'd', 'r', 'o', 'w', 'n', 'g', 'p', 's', 'a', 'x', 't', 'y', 'u', 'h', 'c', 'm', 'b', 'z', 'v']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN3OhFUhFsX4"
      },
      "source": [
        "# MAKING BPNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n82zvJyqmW73"
      },
      "source": [
        "#melakukan vectorize untuk membuat bag of words\n",
        "#bag of words: adalah untuk memetakan(mapping) kata ke dalam bentuk vektor\n",
        "#atau dalam bentuk angka(1 dan 0) dan juga untuk mengecek apakah fitur(data)\n",
        "#yang sudah kita preprocess mengandung kata di dalamnya, jika ada satu kata di dalam fitur tersedia maka akan terhitung 1 , jika tidak maka 0.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vector = CountVectorizer(max_features=10000)\n",
        "bag_words = vector.fit_transform(cleanedWords).toarray()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlwiJaxpv0Np"
      },
      "source": [
        "#splitting dataset menjadi training dan test set dari countvectorization\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(bag_words,np.asarray(df[\"Label\"]), test_size=0.2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2EaYqVuGMe9",
        "outputId": "b25eecca-f89d-4ac8-a184-197113bd8300"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsTn-n4EGTrI",
        "outputId": "15804f64-e7cc-41c5-b463-0c60ebd5fc61"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj1H_JsWGWoj",
        "outputId": "be1ed236-ecbd-45e6-80f1-bab08da55143"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTG6JFj5GhFa",
        "outputId": "b4bb59bd-9305-4c00-de8c-82322e9a2240"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzm7KCX_HonL"
      },
      "source": [
        "#reshape data untuk di apply ke dalam encoder\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFugLMPNbV2"
      },
      "source": [
        "#apply OneHotEncoder ke dalam y_train dan y_test yang sudah di resize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "en = OneHotEncoder()\n",
        "y_train = en.fit_transform(y_train)\n",
        "y_test = en.transform(y_test)\n",
        "y_train = y_train.toarray()\n",
        "y_test = y_test.toarray()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-bGkj4xhq-"
      },
      "source": [
        "#membuat epoch dan learning rate\n",
        "epochs = 2500\n",
        "lr = 0.01"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaTfUo_3ysNl"
      },
      "source": [
        "#membuat placeholder dengan float 32\n",
        "input_tensor = tf.placeholder(tf.float32, name='input')\n",
        "label_tensor = tf.placeholder(tf.float32, name='output')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdlmRkRwyn-3"
      },
      "source": [
        "#membentuk parameter dan arsitektur BPNN\n",
        "#pemakaian angka 64 pada tiap weight dan bias merupakan angka default\n",
        "parameters = {\n",
        "    'W1': tf.Variable(\n",
        "        tf.random.normal([\n",
        "            54, 64\n",
        "        ]),\n",
        "        dtype=tf.float32\n",
        "    ),\n",
        "    'B1': tf.Variable(\n",
        "        tf.random.normal([\n",
        "            1, 64\n",
        "        ]),\n",
        "        dtype=tf.float32\n",
        "    ),\n",
        "    'W2': tf.Variable(\n",
        "        tf.random.normal([\n",
        "        64, 64\n",
        "        ]),\n",
        "        dtype=tf.float32\n",
        "    ),\n",
        "    'B2': tf.Variable(\n",
        "        tf.random.normal([\n",
        "        1, 64\n",
        "        ]),\n",
        "        dtype=tf.float32\n",
        "    ),\n",
        "    'W3': tf.Variable(\n",
        "        tf.random.normal([\n",
        "        64,2\n",
        "        ]),\n",
        "        dtype=tf.float32\n",
        "    ),\n",
        "    'B3': tf.Variable(\n",
        "        tf.random.normal([\n",
        "            1, 2\n",
        "        ]),\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqFRA1xt15kA"
      },
      "source": [
        "#memuat function feed forward dengan activation function sigmoid dan softmax\n",
        "def forward(x, parameters):\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['B1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['B2']\n",
        "  W3 = parameters['W3']\n",
        "  b3 = parameters['B3']\n",
        "\n",
        "  a1 = tf.matmul(x, W1) + b1\n",
        "  z1 = tf.nn.sigmoid(a1)\n",
        "  a2 = tf.matmul(z1, W2) + b2\n",
        "  z2 = tf.nn.softmax(a2)\n",
        "  a3 = tf.matmul(z2, W3) + b3\n",
        "  z3 = tf.nn.softmax(a3)\n",
        "\n",
        "  return a3"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Rrt5Rva7GC"
      },
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnGlI5YR2Asg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5ef121-24c9-40e3-852c-4451d8e46771"
      },
      "source": [
        "#training model dengan  acc, val_acc, val_loss, loss\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  output_tensor = forward(input_tensor, parameters)\n",
        "  loss_tensor = tf.reduce_mean(0.5 * (label_tensor - output_tensor) ** 2)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss_tensor)\n",
        "  true_preds_tensor = tf.equal(\n",
        "      tf.argmax(output_tensor, axis=1), tf.argmax(label_tensor, axis=1)\n",
        "  )\n",
        "  acc_tensor = tf.reduce_mean(tf.cast(true_preds_tensor, tf.float32))\n",
        "  \n",
        "  best_val_loss = 1000.0\n",
        "  for epoch in range(epochs):\n",
        "      sess.run(\n",
        "          optimizer,\n",
        "          feed_dict={\n",
        "              input_tensor: x_train,\n",
        "              label_tensor: y_train\n",
        "          }\n",
        "      )\n",
        "\n",
        "      if epoch % 25 == 0:\n",
        "        loss = sess.run(loss_tensor, feed_dict={\n",
        "          input_tensor: x_train,\n",
        "          label_tensor: y_train\n",
        "        })\n",
        "\n",
        "        acc = sess.run(acc_tensor, feed_dict={\n",
        "        input_tensor: x_train,\n",
        "        label_tensor: y_train\n",
        "    })\n",
        "\n",
        "        print(f'Epoch: {epoch}, loss : {loss}, acc: {acc}')\n",
        "\n",
        "      if epoch % 125 == 0:\n",
        "        val_loss = sess.run(loss_tensor, feed_dict={\n",
        "          input_tensor: x_test,\n",
        "          label_tensor: y_test\n",
        "        })\n",
        "        val_acc = sess.run(acc_tensor, feed_dict={\n",
        "        input_tensor: x_test,\n",
        "        label_tensor: y_test\n",
        "        })\n",
        "\n",
        "        print(f'\\nVal_loss: {val_loss}, val_acc: {val_acc}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          saver.save(sess, './model/best_model.ckpt')\n",
        "          print(\"Model saved!\\n\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch: 0, loss : 2.0765209197998047, acc: 0.5\n",
            "\n",
            "Val_loss: 1.4463896751403809, val_acc: 0.5\n",
            "Model saved!\n",
            "\n",
            "Epoch: 25, loss : 1.3390679359436035, acc: 0.5\n",
            "Epoch: 50, loss : 0.7882785201072693, acc: 0.5\n",
            "Epoch: 75, loss : 0.5345466136932373, acc: 0.5\n",
            "Epoch: 100, loss : 0.3903392255306244, acc: 0.75\n",
            "Epoch: 125, loss : 0.2968374788761139, acc: 0.75\n",
            "\n",
            "Val_loss: 0.4107763469219208, val_acc: 0.5\n",
            "Model saved!\n",
            "\n",
            "Epoch: 150, loss : 0.23258736729621887, acc: 0.75\n",
            "Epoch: 175, loss : 0.18650972843170166, acc: 0.75\n",
            "Epoch: 200, loss : 0.1523474007844925, acc: 0.75\n",
            "Epoch: 225, loss : 0.12633869051933289, acc: 0.75\n",
            "Epoch: 250, loss : 0.10606130957603455, acc: 0.75\n",
            "\n",
            "Val_loss: 0.20139645040035248, val_acc: 0.5\n",
            "Model saved!\n",
            "\n",
            "Epoch: 275, loss : 0.08985374122858047, acc: 0.75\n",
            "Epoch: 300, loss : 0.07656005024909973, acc: 0.75\n",
            "Epoch: 325, loss : 0.0654270127415657, acc: 1.0\n",
            "Epoch: 350, loss : 0.05602692812681198, acc: 1.0\n",
            "Epoch: 375, loss : 0.04814038798213005, acc: 1.0\n",
            "\n",
            "Val_loss: 0.14511516690254211, val_acc: 0.5\n",
            "Model saved!\n",
            "\n",
            "Epoch: 400, loss : 0.0416206456720829, acc: 1.0\n",
            "Epoch: 425, loss : 0.0363038033246994, acc: 1.0\n",
            "Epoch: 450, loss : 0.031994327902793884, acc: 1.0\n",
            "Epoch: 475, loss : 0.028491593897342682, acc: 1.0\n",
            "Epoch: 500, loss : 0.025617320090532303, acc: 1.0\n",
            "\n",
            "Val_loss: 0.13103660941123962, val_acc: 0.5\n",
            "Model saved!\n",
            "\n",
            "Epoch: 525, loss : 0.023227615281939507, acc: 1.0\n",
            "Epoch: 550, loss : 0.021212104707956314, acc: 1.0\n",
            "Epoch: 575, loss : 0.019487937912344933, acc: 1.0\n",
            "Epoch: 600, loss : 0.017993465065956116, acc: 1.0\n",
            "Epoch: 625, loss : 0.016682462766766548, acc: 1.0\n",
            "\n",
            "Val_loss: 0.12962351739406586, val_acc: 0.5\n",
            "Model saved!\n",
            "\n",
            "Epoch: 650, loss : 0.015519974753260612, acc: 1.0\n",
            "Epoch: 675, loss : 0.014479421079158783, acc: 1.0\n",
            "Epoch: 700, loss : 0.013540303334593773, acc: 1.0\n",
            "Epoch: 725, loss : 0.01268655527383089, acc: 1.0\n",
            "Epoch: 750, loss : 0.01190570741891861, acc: 1.0\n",
            "\n",
            "Val_loss: 0.13175319135189056, val_acc: 0.5\n",
            "Epoch: 775, loss : 0.011187727563083172, acc: 1.0\n",
            "Epoch: 800, loss : 0.010524563491344452, acc: 1.0\n",
            "Epoch: 825, loss : 0.009909712709486485, acc: 1.0\n",
            "Epoch: 850, loss : 0.009337801486253738, acc: 1.0\n",
            "Epoch: 875, loss : 0.008804344572126865, acc: 1.0\n",
            "\n",
            "Val_loss: 0.13482598960399628, val_acc: 0.5\n",
            "Epoch: 900, loss : 0.008305689319968224, acc: 1.0\n",
            "Epoch: 925, loss : 0.007838619872927666, acc: 1.0\n",
            "Epoch: 950, loss : 0.007400423754006624, acc: 1.0\n",
            "Epoch: 975, loss : 0.006988789886236191, acc: 1.0\n",
            "Epoch: 1000, loss : 0.006601687520742416, acc: 1.0\n",
            "\n",
            "Val_loss: 0.13809682428836823, val_acc: 0.5\n",
            "Epoch: 1025, loss : 0.00623729545623064, acc: 1.0\n",
            "Epoch: 1050, loss : 0.005894015077501535, acc: 1.0\n",
            "Epoch: 1075, loss : 0.0055704265832901, acc: 1.0\n",
            "Epoch: 1100, loss : 0.0052652666345238686, acc: 1.0\n",
            "Epoch: 1125, loss : 0.004977341741323471, acc: 1.0\n",
            "\n",
            "Val_loss: 0.14133167266845703, val_acc: 0.5\n",
            "Epoch: 1150, loss : 0.004705611150711775, acc: 1.0\n",
            "Epoch: 1175, loss : 0.004449065309017897, acc: 1.0\n",
            "Epoch: 1200, loss : 0.004206853918731213, acc: 1.0\n",
            "Epoch: 1225, loss : 0.003978096880018711, acc: 1.0\n",
            "Epoch: 1250, loss : 0.003762032836675644, acc: 1.0\n",
            "\n",
            "Val_loss: 0.14444510638713837, val_acc: 0.5\n",
            "Epoch: 1275, loss : 0.003557950258255005, acc: 1.0\n",
            "Epoch: 1300, loss : 0.0033651695121079683, acc: 1.0\n",
            "Epoch: 1325, loss : 0.003183056367561221, acc: 1.0\n",
            "Epoch: 1350, loss : 0.00301101035438478, acc: 1.0\n",
            "Epoch: 1375, loss : 0.0028484731446951628, acc: 1.0\n",
            "\n",
            "Val_loss: 0.14738933742046356, val_acc: 0.5\n",
            "Epoch: 1400, loss : 0.0026949227321892977, acc: 1.0\n",
            "Epoch: 1425, loss : 0.002549841534346342, acc: 1.0\n",
            "Epoch: 1450, loss : 0.002412796951830387, acc: 1.0\n",
            "Epoch: 1475, loss : 0.0022833102848380804, acc: 1.0\n",
            "Epoch: 1500, loss : 0.0021609815303236246, acc: 1.0\n",
            "\n",
            "Val_loss: 0.15012910962104797, val_acc: 0.5\n",
            "Epoch: 1525, loss : 0.002045416971668601, acc: 1.0\n",
            "Epoch: 1550, loss : 0.001936221495270729, acc: 1.0\n",
            "Epoch: 1575, loss : 0.0018330734455958009, acc: 1.0\n",
            "Epoch: 1600, loss : 0.0017356049502268434, acc: 1.0\n",
            "Epoch: 1625, loss : 0.001643530442379415, acc: 1.0\n",
            "\n",
            "Val_loss: 0.15263792872428894, val_acc: 0.5\n",
            "Epoch: 1650, loss : 0.0015565267531201243, acc: 1.0\n",
            "Epoch: 1675, loss : 0.0014743184437975287, acc: 1.0\n",
            "Epoch: 1700, loss : 0.0013966355472803116, acc: 1.0\n",
            "Epoch: 1725, loss : 0.00132322171702981, acc: 1.0\n",
            "Epoch: 1750, loss : 0.0012538442388176918, acc: 1.0\n",
            "\n",
            "Val_loss: 0.15490131080150604, val_acc: 0.5\n",
            "Epoch: 1775, loss : 0.0011882837861776352, acc: 1.0\n",
            "Epoch: 1800, loss : 0.0011263277847319841, acc: 1.0\n",
            "Epoch: 1825, loss : 0.001067753997631371, acc: 1.0\n",
            "Epoch: 1850, loss : 0.0010123875690624118, acc: 1.0\n",
            "Epoch: 1875, loss : 0.0009600429912097752, acc: 1.0\n",
            "\n",
            "Val_loss: 0.15691791474819183, val_acc: 0.5\n",
            "Epoch: 1900, loss : 0.0009105474455282092, acc: 1.0\n",
            "Epoch: 1925, loss : 0.0008637467981316149, acc: 1.0\n",
            "Epoch: 1950, loss : 0.0008194900583475828, acc: 1.0\n",
            "Epoch: 1975, loss : 0.0007776265265420079, acc: 1.0\n",
            "Epoch: 2000, loss : 0.0007380347233265638, acc: 1.0\n",
            "\n",
            "Val_loss: 0.15869714319705963, val_acc: 0.5\n",
            "Epoch: 2025, loss : 0.0007005747756920755, acc: 1.0\n",
            "Epoch: 2050, loss : 0.0006651260191574693, acc: 1.0\n",
            "Epoch: 2075, loss : 0.0006315817008726299, acc: 1.0\n",
            "Epoch: 2100, loss : 0.0005998311680741608, acc: 1.0\n",
            "Epoch: 2125, loss : 0.0005697828601114452, acc: 1.0\n",
            "\n",
            "Val_loss: 0.16025491058826447, val_acc: 0.5\n",
            "Epoch: 2150, loss : 0.0005413286853581667, acc: 1.0\n",
            "Epoch: 2175, loss : 0.0005143836024217308, acc: 1.0\n",
            "Epoch: 2200, loss : 0.000488868507090956, acc: 1.0\n",
            "Epoch: 2225, loss : 0.0004646932939067483, acc: 1.0\n",
            "Epoch: 2250, loss : 0.00044180042459629476, acc: 1.0\n",
            "\n",
            "Val_loss: 0.1616138517856598, val_acc: 0.5\n",
            "Epoch: 2275, loss : 0.0004201008123345673, acc: 1.0\n",
            "Epoch: 2300, loss : 0.00039953819941729307, acc: 1.0\n",
            "Epoch: 2325, loss : 0.00038004337693564594, acc: 1.0\n",
            "Epoch: 2350, loss : 0.000361565500497818, acc: 1.0\n",
            "Epoch: 2375, loss : 0.0003440417640376836, acc: 1.0\n",
            "\n",
            "Val_loss: 0.16279715299606323, val_acc: 0.5\n",
            "Epoch: 2400, loss : 0.00032742085750214756, acc: 1.0\n",
            "Epoch: 2425, loss : 0.0003116533625870943, acc: 1.0\n",
            "Epoch: 2450, loss : 0.0002966980682685971, acc: 1.0\n",
            "Epoch: 2475, loss : 0.000282504188362509, acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xb9Ou99uBMM",
        "outputId": "132543a0-1129-40e7-e6b1-61b5360b6ebe"
      },
      "source": [
        "#bagian ini adalah bagian untuk melihat value yang dihasilkan dari model\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  logits_tensor = forward(input_tensor,parameters)\n",
        "  loss_tensor = tf.reduce_mean(0.5 * (label_tensor - logits_tensor)**2)\n",
        "  for s in range(epoch):\n",
        "    sess.run(optimizer, feed_dict={\n",
        "        input_tensor: x_train,\n",
        "        label_tensor: y_train\n",
        "    })\n",
        "    \n",
        "    train_predicted=sess.run(logits_tensor, feed_dict={\n",
        "          input_tensor: x_train,\n",
        "          label_tensor: y_train\n",
        "        })\n",
        "    predicted=sess.run(logits_tensor, feed_dict={\n",
        "          input_tensor: x_test,\n",
        "          label_tensor: y_test\n",
        "        })\n",
        "    \n",
        "true_value = np.argmax(y_test, 1)\n",
        "print(\"true value from test: \", true_value)\n",
        "predicted = np.argmax(predicted, 1)\n",
        "print(\"predicted true value from test: \", predicted)\n",
        "true_value_from_train = np.argmax(y_train,1)\n",
        "print(\"true value from train: \", true_value_from_train)\n",
        "predicted_true_value_from_train = np.argmax(train_predicted,1)\n",
        "print(\"predicted true value from train: \", predicted_true_value_from_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true value from test:  [1 0]\n",
            "predicted true value from test:  [1 0]\n",
            "true value from train:  [1 1 0 0]\n",
            "predicted true value from train:  [1 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plc_KcRNIQL8"
      },
      "source": [
        "# PERFORMANCE OF BPNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQE_mcZbx2Tl"
      },
      "source": [
        "# mengimport library dari scikitlearn untuk accuracy score, percision dan recall score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxfrdN6EyBbM",
        "outputId": "96fefc97-7ba7-4ed2-9454-d8bd0509d72c"
      },
      "source": [
        "#melihat hasil precsion dari training\n",
        "Precision_from_training = precision_score(true_value_from_train, predicted_true_value_from_train)\n",
        "print(\"Precision in training data: \", Precision_from_training)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision in training data:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32GsrLzS8qhI",
        "outputId": "a00c458d-b693-4f40-9a6c-cbe17b2da42d"
      },
      "source": [
        "#mealihat hasil dari recall training\n",
        "recall_from_training = recall_score(true_value_from_train, predicted_true_value_from_train)\n",
        "print(\"Recall in training data: \", recall_from_training)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall in training data:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBULqDha89JQ",
        "outputId": "1e514d58-5f36-40cc-8dc4-85ff2a239445"
      },
      "source": [
        "#mealihat hasil dari accuracy training\n",
        "accuracy_from_training = accuracy_score(true_value_from_train, predicted_true_value_from_train)\n",
        "print(\"Accuracy in training data: \",accuracy_from_training)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy in training data:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5IDHvwt9Nyh",
        "outputId": "e9ed6a96-79ed-483b-d393-78e49eaa1767"
      },
      "source": [
        "#melihat hasil dari precision test\n",
        "precision_test = precision_score(true_value, predicted)\n",
        "print(f'Precision on test data: {precision_test:.3f}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision on test data: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpy4jvEy9uaa",
        "outputId": "f424c1e1-cc32-4342-a643-8845603d4bf2"
      },
      "source": [
        "#melihat hasil dari recall test\n",
        "recall_test = recall_score(true_value, predicted)\n",
        "print(f'Precision on test data: {recall_test:.3f}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision on test data: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3teOSbbw9uZj",
        "outputId": "fc837f40-6f4c-4fa4-9f8c-d47442d93838"
      },
      "source": [
        "#melihat accuracy dari test\n",
        "accuracy_test = accuracy_score(true_value, predicted)\n",
        "print(f'Precision on test data: {accuracy_test:.3f}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision on test data: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}